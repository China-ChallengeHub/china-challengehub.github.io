[{"categories":["技术博客"],"content":"1 图学习任务 我们简单回顾下，上一节我们介绍了，图的机器学习任务主要是以下三种：\n Node Level:节点级别 Link Level：边级别 Graph Level：图级别å 并且三部分难度依次是由浅入深的   2 传统ML流程  定义和设计节点/边/图的特征 对所有训练数据构造特征  训练ML模型 （1）随机森林 （2）支持向量机 （3）神经网络等 应用模型 给定一个新的节点、边、图，然后获取特征进行预测  我们总结下 基于Graph的机器学习相关概念和流程，首先明确下目标\n目标：对一些对象集合进行预测，比如是分类或者回归任务 特征设计：\n 特征:d-dimensional向量 对象：Nodes，edges，或者是graps 目标函数：结合具体任务设计目标函数，如下图所示给定一个图G(V,E)，其中V代表节点集合，E代表边集合，然后学习节点到向量空间R的映射函数，也就是我们要学习权重参数W    为了方便，我们下面的例子是基于无向图(undirected grpah)进行解释的。\n 3 节点级别的相关任务 基于图中带有标签的节点训练模型，然后预测未标注节点的标签， 在这里我们主要阐述下Node的四种特征：\n Node degree:节点的度 Node centrality:节点的中度 Clustering coefficient:相似性 Graphlets:图元   节点的度\n kv代表是节点v与邻居节点相连边的个数 所有邻居节点都是相等的  如下图所示，A的度为1，B的度为2，C的度为3，D的度为4 节点的中心度 Node Centrality\n 节点的度只计算了相连节点的个数，但是没有评估节点的重要性 节点的中心度$c_v$考虑了节点在图中的重要程度 节点的中心度有很多种计算方式： (1) Engienvector centrality:特征向量中心性 (2) Betweenness centrality:中介中心性 (3) Closeness centrality:紧密中心性 (4) 其他方式  Eigenvector centrality:特征向量中心性\n 如果一个节点的邻居节点们$u\\in N(v)$越重要，那么该节点$v$就越重要 我们将节点𝑣的中心性建模为相邻节点的中心性之和： $c_v=\\frac{1}{\\lambda}\\sum_{u \\in N(v)}{c_u}$ 其中$\\lambda$为一个正常数。 我们注意到上面的等式是通过递归的方式来计算度度中心性的，所有我们应该怎么求解  $c_v=\\frac{1}{\\lambda}\\sum_{u \\in N(v)}{c_u} \\iff \\lambda c=Ac$\n其中$\\lambda$为正常数，$A$为邻接矩阵，如果$u \\in N(v)$ ,那么$A_{uv}=1$\n 我们可以看到度中心性是一个特征向量(eigenvector)，根据非负矩阵定理(Perron-Frobenius Theorem)我们可以知道$\\lambda_{max}$是一个正值并且是唯一的，特征向量$c_{max}$当做度中心性。  通常来说，有许多不同的特征值$\\lambda$ 能使得一个特征方程有非零解存在。然而，考虑到特征向量中的所有项均为非负值，根据佩伦-弗罗贝尼乌斯定理，只有特征值最大时才能测量出想要的中心性。然后通过计算网络中的节点$v$其特征向量的相关分量$v^{th}$便能得出其对应的中心性的分数。\n 特征向量的定义只有一个公因子，因此各节点中心性的比例可以很好确定。为了确定一个绝对分数，必须将其中一个特征值标准化，例如所有节点评分之和为1或者节点数 n。幂次迭代是许多特征值算法中的一种，该算法可以用来寻找这种主导特征向量。此外，以上方法可以推广，使得矩阵A中每个元素可以是表示连接强度的实数，例如随机矩阵。\u0026mdash;特征向量中心性wiki\n 这里其实涉及到比较多的线性代数的理论以及矩阵分析的算法，我特意查了一些文章帮大家去回顾和理解下这里涉及到的知识： (1) 线性代数之——特征值和特征向量 (2)非负矩阵之Perron-Frobenius定理 (3)非负矩阵 (4)干货 | 万字长文带你复习线性代数！  我们这里再通过文章谁是社会网络中最重要的人？ 解释特征向量中心性： 特征向量中心性的基本思想是，一个节点的中心性是相邻节点中心性的函数。也就是说，与你连接的人越重要，你也就越重要。\n特征向量中心性和点度中心性不同，一个点度中心性高即拥有很多连接的节点特征向量中心性不一定高，因为所有的连接者有可能特征向量中心性很低。同理，特征向量中心性高并不意味着它的点度中心性高，它拥有很少但很重要的连接者也可以拥有高特征向量中心性。\n考虑下面的图，以及相应的5x5的邻接矩阵(Adjacency Matrix)，A。\n邻接矩阵的含义是，如果两个节点没有直接连接，记为0，否则记为1。\n现在考虑x，一个5x1的向量，向量的值对应图中的每个点。在这种情况下，我们计算的是每个点的点度中心性（degree centrality），即以点的连接数来衡量中心性的高低。\n矩阵A乘以这个向量的结果是一个5x1的向量：\n结果向量的第一个元素是用矩阵A的第一行去“获取”每一个与第一个点有连接的点的值（连接数，点度中心性），也就是第2个、第3个和第4个点的值，然后将它们加起来。\n换句话说，邻接矩阵做的事情是将相邻节点的求和值重新分配给每个点。这样做的结果就是“扩散了”点度中心性。你的朋友的朋友越多，你的特征向量中心性就越高。\n我们继续用矩阵A乘以结果向量。如何理解呢？实际上，我们允许这一中心性数值再次沿着图的边界“扩散”。我们会观察到两个方向上的扩散（点既给予也收获相邻节点）。我们猜测，这一过程最后会达到一个平衡，特定点收获的数量会和它给予相邻节点的数量取得平衡。既然我们仅仅是累加，数值会越来越大，但我们最终会到达一个点，各个节点在整体中的比例会保持稳定。\n现在把所有点的数值构成的向量用更一般的形式表示：\n我们认为，图中的点存在一个数值集合，对于它，用矩阵A去乘不会改变向量各个数值的相对大小。也就是说，它的数值会变大，但乘以的是同一个因子。用数学符号表示就是：\n满足这一属性的向量就是矩阵M的特征向量。特征向量的元素就是图中每个点的特征向量中心性。\n特征向量中心性的计算需要读者具备矩阵乘法和特征向量的知识，但不影响这里读者对特征向量中心性思想的理解，不再赘述。\nBetweenness centrality:中介中心性 如果一个节点位于很多条其他节点的最短路径上，那么改节点比较重要，定义如下： $c_{v}=\\sum_{s\\not=v\\not=t}{\\frac{#(shortes,paths ,betwen ,𝑠 , and , 𝑡 , that ,contain ,𝑣)}{#(shortest ,paths ,between ,\u0026lsquo;𝑠\u0026rsquo; ,and ,\u0026lsquo;𝑡\u0026rsquo;)}}$ 我们可以看一个下面的例子： 假设我们要计算D的中介中心性：\n 首先，我们计算节点D之外，所有节点对之间的最短路径有多少条，这里是1条（在5个节点中选择两个节点即节点对的个数）。 然后，我们再看所有这些最短路径中有多少条经过节点D，例如节点A要想找到节点E，必须经过节点D。经过节点D的最短路径有3条（A-C-D-E，B-D-E，C-D-E）。 最后，我们用经过节点D的最短路径除以所有节点对的最短路径总数，这个比率就是节点D的中介中心性。节点D的中介中心性是3/1=3。  4 PyTorch Geometric教程 官方文档：https://pytorch-geometric.readthedocs.io/en/latest/index.html\nPyTorch Geometric（PyG）是PyTorch的扩展库。 它提供了有用的框架来开发图深度学习模型，包括各种图神经网络层和大量基准数据集。\n如果我们暂时不了解某些概念，例如“ GCNConv”，请不要担心，之后我们将在以后的文章中介绍这些概念。\n这个教程来自： https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=ci-LpZWhRJoI by Matthias Fey 大家可以参照colab教程直接运行\n4.1 PyG安装  查看当前torch和cuda版本  !python -c \u0026quot;import torch; print(torch.__version__)\u0026quot; 输出：1.6.0\n!python -c \u0026quot;import torch; print(torch.version.cuda)\u0026quot; 输出：10.1\n 安装gpu版本  # 安装GPU版本 !pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html !pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html !pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html !pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html !pip install torch-geometric 可能安装gpu版本比较麻烦，大家可以多参照下官网进行安装： https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n 安装cuda版本  !pip install torch-scatter==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.6.0.html !pip install torch-sparse==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.6.0.html !pip install torch-cluster==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.6.0.html !pip install torch-spline-conv==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.6.0.html !pip install torch-geometric 4.2 PyG数据集 PyTorch Geometric 可以通过 torch_geometric.datasets 快速的获取默认的数据集:\nhttps://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n一共有20+种数据，比较丰富~\n# 导入空手道俱乐部数据集 from torch_geometric.datasets import KarateClub dataset = KarateClub() print(f'Dataset: {dataset}:') print('======================') print(f'Number of graphs: {len(dataset)}') print(f'Number of features: {dataset.num_features}') print(f'Number of classes: {dataset.num_classes}') 输出：\nDataset: KarateClub(): ====================== Number of graphs: 1 Number of features: 34 Number of classes: 4 初始化[KarateClub]（https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub） 数据集之后，我们首先可以检查其某些属性。 例如，我们可以看到该数据集恰好有一个Graph，并且该数据集中的每个节点都被分配了** 34维特征向量**（唯一地描述了空手道俱乐部的成员）。 此外，该图正好包含** 4个类别**，代表每个节点所属的社区。\n现在让我们更详细地看一下这个Graph：\ndata = dataset[0] # 获取graph对象. print(data) print(\u0026#39;==============================================================\u0026#39;) # 获取一些graph的统计信息. print(f\u0026#39;Number of nodes: {data.num_nodes}\u0026#39;) # 节点的个数 print(f\u0026#39;Number of edges: {data.num_edges}\u0026#39;) # 边的个属于 print(f\u0026#39;Average node degree: {data.num_edges / data.num_nodes:.2f}\u0026#39;) # 节点的度平均数 print(f\u0026#39;Number of training nodes: {data.train_mask.sum()}\u0026#39;) print(f\u0026#39;Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}\u0026#39;) print(f\u0026#39;Contains isolated nodes: {data.contains_isolated_nodes()}\u0026#39;) print(f\u0026#39;Contains self-loops: {data.contains_self_loops()}\u0026#39;) print(f\u0026#39;Is undirected: {data.is_undirected()}\u0026#39;) 输出如下：\n Data(edge_index=[2, 156], train_mask=[34], x=[34, 34], y=[34]) ============================================================== Number of nodes: 34 Number of edges: 156 Average node degree: 4.59 Number of training nodes: 4 Training node label rate: 0.12 Contains isolated nodes: False Contains self-loops: False Is undirected: True 通过上面的信息我们可以发现，这个KarateClub网络图一共有34个节点，边的个数为1，每个节点度的平均数有4.59，是一个无向图等等。\n4.3 Data对象 PyTorch Geometric中的每个图形都由单个[Data]（https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data）对象表示，该对象包含所有 描述其图形表示的信息。 我们可以随时通过print（data）打印数据对象，以接收有关其属性及其形状的简短介绍：\nData(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34]) 我们可以看到这个data对象拥有4个属性： （1）“ edge_index”属性保存有关“图形连接性”（即*）的信息，即每个边缘的源节点和目标节点索引的元组。 PyG进一步将 （2）节点特征**称为x（向34个节点中的每个节点分配了34维度的特征向量），将 （3）节点标记**称为y（每个节点仅分配给一个类别）。 （4）还有一个名为“ train_mask”的附加属性，它描述了我们已经知道其社区归属的节点。 总体而言，我们只知道4个节点的基本标签（每个社区一个），任务是推断其余节点的社区分配。\n数据对象还提供了一些“实用功能”来推断基础图的一些基本属性。 例如，我们可以轻松推断图中是否存在孤立的节点（* ie *任何节点都没有边），图是否包含自环(*i.e.*, $(v, v) \\in \\mathcal{E}$)，或者图形是否是无向的（ (*i.e.*, for each edge $(v, w) \\in \\mathcal{E}$ there also exists the edge $(w, v) \\in \\mathcal{E}$).\nfrom IPython.display import Javascript # Restrict height of output cell. display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})''')) edge_index = data.edge_index # 打印节点 print(edge_index.t()) 输出：\ntensor([[ 0, 1], [ 0, 2], [ 0, 3], [ 0, 4], [ 0, 5], [ 0, 6], [ 0, 7], [ 0, 8], [ 0, 10], .... [33, 31], [33, 32]]) 通过打印“ edge_index”，我们可以进一步了解PyG如何在内部表示图形连接。 我们可以看到，对于每个边缘，“ edge_index”都包含两个节点索引的元组，其中第一个值描述源节点的节点索引，第二个值描述边缘目标节点的节点索引。\n这种表示称为** COO格式（坐标格式）**，通常用于表示稀疏矩阵。 代替以密集表示形式保存邻接信息 $\\mathbf{A} \\in { 0, 1 }^{|\\mathcal{V}| \\times |\\mathcal{V}|}$,，PyG稀疏地表示图形，这是指仅保存 $\\mathbf{A}$ 中的条目为非零的坐标/值。\n最后，我们可以通过将图形转换为networkx库格式来进一步可视化图形，该格式除了图形操作功能之外，还实现了强大的可视化工具。我们创建一个专门用于展示Graph的函数\n%matplotlib inline import torch import networkx as nx import matplotlib.pyplot as plt # Visualization function for NX graph or PyTorch tensor def visualize(h, color, epoch=None, loss=None): plt.figure(figsize=(7,7)) plt.xticks([]) plt.yticks([]) if torch.is_tensor(h): h = h.detach().cpu().numpy() plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\u0026quot;Set2\u0026quot;) if epoch is not None and loss is not None: plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16) else: nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False, node_color=color, cmap=\u0026quot;Set2\u0026quot;) plt.show() from torch_geometric.utils import to_networkx G = to_networkx(data, to_undirected=True) visualize(G, color=data.y) 4.4 实战：基于GCN的Graph节点分类  接下来将通过Pytorch实现一个最基本的GCN网络用语节点分类，基于带有标签的节点数据进行训练模型，然后预测未带有标签的数据\n 在了解了PyG的数据处理之后，是时候实现我们的第一个Graph神经网络了！我们在这里将使用最简单的GNN网络之一，即GCN层**（[Kipf等人（2017）]（https://arxiv.org/abs/1609.02907））用于Graph节点分类。\nPyG通过[GCNConv]（https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv）来实现此层，可以通过传入 节点要素表示形式“ x”和COO图形连接性表示形式“ edge_index”。\n这样，我们就可以通过在torch.nn.Module类中定义我们的网络架构来创建我们的第一个图形神经网络：\nimport torch from torch.nn import Linear from torch_geometric.nn import GCNConv class GCN(torch.nn.Module): def __init__(self): super(GCN, self).__init__() torch.manual_seed(12345) self.conv1 = GCNConv(dataset.num_features, 4) self.conv2 = GCNConv(4, 4) self.conv3 = GCNConv(4, 2) self.classifier = Linear(2, dataset.num_classes) def forward(self, x, edge_index): h = self.conv1(x, edge_index) h = h.tanh() h = self.conv2(h, edge_index) h = h.tanh() h = self.conv3(h, edge_index) h = h.tanh() # Final GNN embedding space. # Apply a final (linear) classifier. out = self.classifier(h) return out, h model = GCN() print(model) 网络结构输出如下：\nGCN( (conv1): GCNConv(34, 4) (conv2): GCNConv(4, 4) (conv3): GCNConv(4, 2) (classifier): Linear(in_features=2, out_features=4, bias=True) ) 在这里，我们首先在__init__中初始化所有构建块，并在“转发”中定义网络的计算流程。 我们首先定义并堆叠三层图卷积层，这对应于在每个节点周围（距离3个“跳”为止的所有节点）汇总3跳邻域信息。 另外，GCNConv层将节点特征维数减小为$2$, i.e., $34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2$.。每个[GCNConv]层都通过[tanh]（https://pytorch.org/docs/stable/generation/torch.nn.Tanh.html?highlight=tanh#torch.nn.Tanh）非线性进行了增强。\n之后，我们应用单个线性变换（[torch.nn.Linear]（https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn。线性）），用作将我们的节点映射到4个类/社区中的1个的分类器。\n我们返回最终分类器的输出以及GNN生成的最终节点嵌入。 我们继续通过GCN()初始化最终模型，并打印我们的模型以产生所有使用过的子模块的概括。\n获取隐藏层的表示：\nmodel = GCN() _, h = model(data.x, data.edge_index) print(f'Embedding shape: {list(h.shape)}') visualize(h, color=data.y) 输出：\nEmbedding shape: [34, 2] 在这里值得注意的是，即使在训练我们的模型权重之前，这个初始化的GCN网络也会产生节点的嵌入，并且这些嵌入与图的社区结构非常相似。尽管我们的模型的权重是“完全随机地初始化”的，并且到目前为止，我们还没有进行任何训练，但是相同颜色（社区）的节点已经在嵌入空间中紧密地聚集在一起。得出这样的结论，即GNN引入了强烈的节点偏差，从而导致在输入图中彼此靠近的节点具有相似的嵌入。\n但是，节点表示并不是很完美，我们可以看出来四种类别的节点还是混在一块了（对应途中是四种颜色），我们可以做得更好吗？ 让我们看一个示例，该示例如何基于对图中4四种节点的社区分配（每个社区一个）的知识来训练我们的网络参数：\n由于模型中的所有内容都是可区分的和参数化的，因此我们可以添加一些标签，训练模型并观察嵌入如何反应。 在这里，我们使用一种半监督学习程序：我们仅针对每个类训练一个节点，但是允许使用完整的输入图数据。\n这个怎么理解呢，我们其实可以输出data.y和data.train_mask就明白了:\ndata.y tensor([0, 0, 0, 0, 1, 1, 1, 0, 2, 2, 1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 2, 0, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2]) data.train_mask tensor([ True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False]) 大家可以看到相当于我们训练网络的时候只针对每一个类别下的token做优化，这样可以加速网络的训练和收敛。\n训练我们的模型与任何其他PyTorch模型非常相似。 除了定义我们的网络架构之外，我们还定义了损失函数在这里[[CrossEntropyLoss]]（https://pytorch.org/docs/stable/generate/torch.nn.CrossEntropyLoss.html）） 并初始化随机梯度优化器（此处为[Adam]（https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam））。 之后，我们执行多轮优化，其中每一轮都包含一个正向和反向传递，以计算模型参数w.r.t的梯度。从前向通行证产生的损失。 如果您对PyTorch并不陌生，则该方案应该对您来说很熟悉。 否则，PyTorch文档会提供[有关如何在PyTorch中训练神经网络的很好的介绍]（https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer ）。\n请注意，我们的半监督学习场景是通过以下行实现的：\nloss = criterion(out[data.train_mask], data.y[data.train_mask]) 在计算所有节点的节点嵌入时，我们“仅利用训练节点来计算loss” **。 在这里，这是通过过滤分类器“ out”和真实性标签“ data.y”的输出以仅包含“ train_mask”中的节点来实现的。\n现在让我们开始训练，看看我们的节点嵌入随时间如何演变（最好是显式地运行代码，打印出模型的训练效果）：\nimport time from IPython.display import Javascript #画图. display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 430})''')) model = GCN() criterion = torch.nn.CrossEntropyLoss() # 定义loss optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 优化器 def train(data): optimizer.zero_grad() # 梯度清零. out, h = model(data.x, data.edge_index) # GCN模型. loss = criterion(out[data.train_mask], data.y[data.train_mask]) # 计算真实loss. loss.backward() # 反向求导. optimizer.step() # 参数更新. return loss, h for epoch in range(401): loss, h = train(data) # 每10个epochs打印Graph if epoch % 10 == 0: visualize(h, color=data.y, epoch=epoch, loss=loss) time.sleep(0.3) 过了一段时间之后，我们可以看到GCN强大的效果：\n可以看到，我们的3层GCN模型可以有效地社区发现并正确地对大多数节点进行分类。\n","permalink":"/posts/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C04-graph-ml/","series":["图神经网络"],"tags":["图神经网络"],"title":"图神经网络(04)-基于Graph的传统机器学习方法"},{"categories":["技术博客"],"content":"简介 新闻媒体已成为向世界人民传递世界上正在发生的事情的信息的渠道。 人们通常认为新闻中传达的一切都是真实的。 在某些情况下，甚至新闻频道也承认他们的新闻不如他们写的那样真实。 但是，一些新闻不仅对人民或政府产生重大影响，而且对经济也产生重大影响。 一则新闻可以根据人们的情绪和政治局势上下移动曲线。\n从真实的真实新闻中识别虚假新闻非常重要。 该问题已通过自然语言处理工具解决并得到了解决，本篇文章可帮助我们根据历史数据识别假新闻或真实新闻。\n问题描述 对于印刷媒体和数字媒体，信息的真实性已成为影响企业和社会的长期问题。在社交网络上，信息传播的范围和影响以如此快的速度发生，并且如此迅速地放大，以至于失真，不准确或虚假的信息具有巨大的潜力，可在数分钟内对数百万用户造成现实世界的影响。最近，人们表达了对该问题的一些担忧，并提出了一些缓解该问题的方法。\n在各种信息广播的整个历史中，一直存在着不那么精确的引人注目和引人入胜的新闻标题，这些新闻标题旨在吸引观众的注意力来出售信息。但是，在社交网站上，信息传播的范围和影响得到了显着放大，并且发展速度如此之快，以至于失真，不准确或虚假的信息具有巨大的潜力，可在数分钟内为数百万的用户带来真正的影响。\n目标  我们唯一的目标是将数据集中的新闻分类为假新闻或真实新闻。 新闻的细致EDA 选择并建立强大的分类模型  导入相关库 让我们导入所有必要的库以进行文本分析，并且将对数据集进行清洗。\n# 基本数据包：pandas和numpy import pandas as pd import numpy as np # 可视化包 import matplotlib.pyplot as plt from matplotlib import rcParams import seaborn as sns from textblob import TextBlob from plotly import tools import plotly.graph_objs as go from plotly.offline import iplot %matplotlib inline plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = [10, 5] import cufflinks as cf cf.go_offline() cf.set_config_file(offline=False, world_readable=True) # NLTK 包 import nltk import re import string from nltk.corpus import stopwords from wordcloud import WordCloud,STOPWORDS from nltk.stem.porter import PorterStemmer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction.text import CountVectorizer # Machine Learning libraries import sklearn from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # 评估 from sklearn import metrics from sklearn.metrics import classification_report from sklearn.model_selection import cross_val_score from sklearn.metrics import roc_auc_score from sklearn.metrics import roc_curve from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score # 集合 from collections import Counter # 忽略警告 import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # 深度学习 from tensorflow.keras.layers import Embedding from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.preprocessing.text import one_hot from tensorflow.keras.layers import LSTM from tensorflow.keras.layers import Bidirectional from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Dropout 加载数据 数据下载链接：添加链接描述 # 读取数据集 fake_news = pd.read_csv(\u0026#39;data/Fake.csv\u0026#39;) true_news = pd.read_csv(\u0026#39;data/True.csv\u0026#39;) # 虚假新闻数据集的大小以及字段 print (\u0026#34;The shape of the data is (row, column):\u0026#34;+ str(fake_news.shape)) print (fake_news.info()) print(\u0026#34;\\n--------------------------------------- \\n\u0026#34;) # 真实新闻数据集的大小以及字段 print (\u0026#34;The shape of the data is (row, column):\u0026#34;+ str(true_news.shape)) print (true_news.info()) 数据集的详情 数据有2个CSV文件，其中一个数据集包含假新闻，另一个包真新闻，有将近23481个假新闻和21417个真新闻。 文件中列名的描述:\n title-包含新闻标题 text-包含新闻内容/文章 subject-新闻的主题 date-消息发布的日期  数据预处理与文本清洗 在执行EDA并将数据提供给模型之前，我们必须执行某些预处理步骤：\n创建目标列 让我们为假新闻和真新闻创建目标列。在这里，我们将目标值表示为“ 0”（假新闻），“ 1”（真新闻）。\n#假新闻的目标变量 fake_news[\u0026#39;output\u0026#39;]=0 #真新闻的目标变量 true_news[\u0026#39;output\u0026#39;]=1 拼接新闻标题和内容 新闻是将根据标题和文本进行分类。 分开处理新闻标题和内容不会带来任何好处。 因此，我们将两个数据集中的两个列连接起来。\n#合并title与text合并为news fake_news[\u0026#39;news\u0026#39;]=fake_news[\u0026#39;title\u0026#39;]+fake_news[\u0026#39;text\u0026#39;] fake_news=fake_news.drop([\u0026#39;title\u0026#39;, \u0026#39;text\u0026#39;], axis=1) #合并title与text合并为news true_news[\u0026#39;news\u0026#39;]=true_news[\u0026#39;title\u0026#39;]+true_news[\u0026#39;text\u0026#39;] true_news=true_news.drop([\u0026#39;title\u0026#39;, \u0026#39;text\u0026#39;], axis=1) # 重构表格 fake_news = fake_news[[\u0026#39;subject\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;news\u0026#39;,\u0026#39;output\u0026#39;]] true_news = true_news[[\u0026#39;subject\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;news\u0026#39;,\u0026#39;output\u0026#39;]] 将日期列转换为日期时间格式 我们可以使用pd.datetime将日期列转换为所需的日期格式。尤其是在fake_news date列中，让我们检查value_counts（）看看里面有什么。\nffake_news[\u0026#39;date\u0026#39;].value_counts() Out[7]: May 10, 2017 46 May 26, 2016 44 May 5, 2016 44 May 6, 2016 44 May 11, 2016 43 .. November 20, 2017 1 Jun 21, 2015 1 December 11, 2017 1 Apr 2, 2015 1 Jul 19, 2015 1 Name: date, Length: 1681, dtype: int64 如果您注意到，我们在日期列内有链接和新闻标题，这在转换为日期时间格式时会给我们带来麻烦。因此，让我们从列中删除这些记录。\n# 删除含有链接以及Host的数据  fake_news=fake_news[~fake_news.date.str.contains(\u0026#34;http\u0026#34;)] fake_news=fake_news[~fake_news.date.str.contains(\u0026#34;HOST\u0026#34;)] # \u0026#39;\u0026#39;\u0026#39;等效\u0026#39;\u0026#39;\u0026#39; #fake_news=fake_news[fake_news.date.str.contains(\u0026#34;Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\u0026#34;)] 只有假新闻数据集的日期列存在问题。现在，我们将日期列转换为日期时间格式 In [10]: # 将日期列转为时间格式 fake_news[\u0026#39;date\u0026#39;] = pd.to_datetime(fake_news[\u0026#39;date\u0026#39;]) true_news[\u0026#39;date\u0026#39;] = pd.to_datetime(true_news[\u0026#39;date\u0026#39;]) 合并数据集 当我们为模型提供数据集时，我们必须将其作为单个文件提供。因此，最好同时添加真假新闻数据，并对其进行进一步预处理并执行EDA。\nframes = [fake_news, true_news] news_dataset = pd.concat(frames) news_dataset Out[11]: subject\tdate\tnews\toutput 0\tNews\t2017-12-31\tDonald Trump Sends Out Embarrassing New Year’...\t0 1\tNews\t2017-12-31\tDrunk Bragging Trump Staffer Started Russian ...\t0 2\tNews\t2017-12-30\tSheriff David Clarke Becomes An Internet Joke...\t0 3\tNews\t2017-12-29\tTrump Is So Obsessed He Even Has Obama’s Name...\t0 4\tNews\t2017-12-25\tPope Francis Just Called Out Donald Trump Dur...\t0 ...\t...\t...\t...\t... 21412\tworldnews\t2017-08-22\t\u0026#39;Fully committed\u0026#39; NATO backs new U.S. approach...\t1 21413\tworldnews\t2017-08-22\tLexisNexis withdrew two products from Chinese ...\t1 21414\tworldnews\t2017-08-22\tMinsk cultural hub becomes haven from authorit...\t1 21415\tworldnews\t2017-08-22\tVatican upbeat on possibility of Pope Francis ...\t1 21416\tworldnews\t2017-08-22\tIndonesia to buy $1.14 billion worth of Russia...\t1 44888 rows × 4 columns 文本处理 对于任何文本分析应用程序来说，这都是重要的阶段。 新闻中将有很多无用的内容，这可能会阻碍机器学习模型的发展。 除非我们删除它们，否则机器学习模型将无法有效运行。 让我们一步一步走。\n标点符号去除 clean_news=news_dataset.copy() def review_cleaning(text): \u0026#39;\u0026#39;\u0026#39;Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.\u0026#39;\u0026#39;\u0026#39; text = str(text).lower() text = re.sub(\u0026#39;\\[.*?\\]\u0026#39;, \u0026#39;\u0026#39;, text) text = re.sub(\u0026#39;https?://\\S+|www\\.\\S+\u0026#39;, \u0026#39;\u0026#39;, text) text = re.sub(\u0026#39;\u0026lt;.*?\u0026gt;+\u0026#39;, \u0026#39;\u0026#39;, text) text = re.sub(\u0026#39;[%s]\u0026#39; % re.escape(string.punctuation), \u0026#39;\u0026#39;, text) text = re.sub(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;, text) text = re.sub(\u0026#39;\\w*\\d\\w*\u0026#39;, \u0026#39;\u0026#39;, text) return text clean_news[\u0026#39;news\u0026#39;]=clean_news[\u0026#39;news\u0026#39;].apply(lambda x:review_cleaning(x)) clean_news.head() Out[13]: subject\tdate\tnews\toutput 0\tNews\t2017-12-31\tdonald trump sends out embarrassing new year’...\t0 1\tNews\t2017-12-31\tdrunk bragging trump staffer started russian ...\t0 2\tNews\t2017-12-30\tsheriff david clarke becomes an internet joke...\t0 3\tNews\t2017-12-29\ttrump is so obsessed he even has obama’s name...\t0 4\tNews\t2017-12-25\tpope francis just called out donald trump dur...\t0 停用词去除 停止词是一个常用的词(例如“the”，“A”，“an”，“in”)，搜索引擎在为搜索条目建立索引和作为搜索查询的结果检索它们时都忽略它。我们不希望这些词占用数据库中的空间，或占用宝贵的处理时间。为此，我们可以很容易地删除它们，方法是存储一组你认为可以终止单词的单词。python中的NLTK(自然语言工具包)有一个以16种不同语言存储的stopwords列表。\nimport nltk nltk.download(\u0026#39;stopwords\u0026#39;) [nltk_data] Downloading package stopwords to [nltk_data] C:\\Users\\yanqiang\\AppData\\Roaming\\nltk_data... [nltk_data] Unzipping corpora\\stopwords.zip. Out[15]: True In [16]: stop = stopwords.words(\u0026#39;english\u0026#39;) clean_news[\u0026#39;news\u0026#39;] = clean_news[\u0026#39;news\u0026#39;].apply(lambda x: \u0026#39; \u0026#39;.join([word for word in x.split() if word not in (stop)])) clean_news.head() Out[16]: subject\tdate\tnews\toutput 0\tNews\t2017-12-31\tdonald trump sends embarrassing new year’s eve...\t0 1\tNews\t2017-12-31\tdrunk bragging trump staffer started russian c...\t0 2\tNews\t2017-12-30\tsheriff david clarke becomes internet joke thr...\t0 3\tNews\t2017-12-29\ttrump obsessed even obama’s name coded website...\t0 4\tNews\t2017-12-25\tpope francis called donald trump christmas spe...\t0 新闻的事件演变和可视化 在本节中，我们将完成对新闻的探索性数据分析，例如ngram分析，并了解哪些是所有单词，上下文（最有可能在伪造的new中找到）。\n新闻主题数 ax = sns.countplot(x=\u0026#34;subject\u0026#34;, data=clean_news, facecolor=(0, 0, 0, 0), linewidth=5, edgecolor=sns.color_palette(\u0026#34;dark\u0026#34;, 3)) # 设置label与字体大小 ax.set(xlabel=\u0026#39;Type of news\u0026#39;, ylabel=\u0026#39;Number of news\u0026#39;,title=\u0026#39;Count of news type\u0026#39;) ax.xaxis.get_label().set_fontsize(15) ax.yaxis.get_label().set_fontsize(15) 基于真假的新闻主题计数 g = sns.catplot(x=\u0026#34;subject\u0026#34;, col=\u0026#34;output\u0026#34;, data=clean_news, kind=\u0026#34;count\u0026#34;, height=4, aspect=2) # 旋转x轴 g.set_xticklabels(rotation=45) Out[22]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x2308e99f608\u0026gt; 发现：\n 假新闻无处不在，政治和世界新闻除外 真正的新闻只存在于政治和世界新闻中，而且数量很高 这是一个高度偏差的数据集，考虑到数据集的质量较差，我们可以期望更高的准确性，但这并不意味着它是一个好的模型  真假新闻统计 ax=sns.countplot(x=\u0026#34;output\u0026#34;, data=clean_news) # 设置label以及字体大小 ax.set(xlabel=\u0026#39;Output\u0026#39;, ylabel=\u0026#39;Count of fake/true\u0026#39;,title=\u0026#39;Count of fake and true news\u0026#39;) ax.xaxis.get_label().set_fontsize(15) ax.yaxis.get_label().set_fontsize(15) 发现：\n 我们有一个非常平衡的数据 但是，假新闻的数量要比真实新闻的数量高，但程度不大  从新闻中提取新特征 让我们从新闻特征中提取更多的特征，比如\n 极性:表示新闻情感的尺度 评论长度:新闻的长度(字母和空格的数量) 单词数:新闻中单词的数量  # 从新闻中提取新特征 clean_news[\u0026#39;polarity\u0026#39;] = clean_news[\u0026#39;news\u0026#39;].map(lambda text: TextBlob(text).sentiment.polarity) clean_news[\u0026#39;review_len\u0026#39;] = clean_news[\u0026#39;news\u0026#39;].astype(str).apply(len) clean_news[\u0026#39;word_count\u0026#39;] = clean_news[\u0026#39;news\u0026#39;].apply(lambda x: len(str(x).split())) # 新特征分布 plt.figure(figsize = (20, 5)) plt.style.use(\u0026#39;seaborn-white\u0026#39;) plt.subplot(131) sns.distplot(clean_news[\u0026#39;polarity\u0026#39;]) fig = plt.gcf() plt.subplot(132) sns.distplot(clean_news[\u0026#39;review_len\u0026#39;]) fig = plt.gcf() plt.subplot(133) sns.distplot(clean_news[\u0026#39;word_count\u0026#39;]) fig = plt.gcf() 发现：\n 大部分极性是中性的，既不表示坏消息也不表示高兴消息。 字数在0到1000之间，新闻的长度在0到5000之间，并且接近1000个单词，这可能是一篇文章。  N-gram分析 新闻中的前20个词 让我们看一下新闻中的前20个词，这可以让我们简要了解一下数据集中最受欢迎的新闻。\n# 获取topn的词 def get_top_n_words(corpus, n=None): vec = CountVectorizer().fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] # 获取top20常见的词 common_words = get_top_n_words(clean_news[\u0026#39;news\u0026#39;], 20) # 打印词频 for word, freq in common_words: print(word, freq) # 创建词与词频的dataframe df1 = pd.DataFrame(common_words, columns = [\u0026#39;news\u0026#39; , \u0026#39;count\u0026#39;]) df1.groupby(\u0026#39;news\u0026#39;).sum()[\u0026#39;count\u0026#39;].sort_values(ascending=False).iplot( kind=\u0026#39;bar\u0026#39;, yTitle=\u0026#39;Count\u0026#39;, linecolor=\u0026#39;black\u0026#39;, title=\u0026#39;Top 20 words in news\u0026#39;) trump 140400 said 130258 us 68081 would 55422 president 53189 people 41718 one 36146 state 33190 new 31799 also 31209 obama 29881 clinton 29003 house 28716 government 27392 donald 27376 reuters 27348 states 26331 republican 25287 could 24356 white 23823 发现：\n 所有前20条新闻都与美国政府有关 特别是关于特朗普和美国，其次是奥巴马 我们可以了解到，新闻来自路透社。  新闻中的topn的2个词组合 现在，让我们将探索范围扩展到新闻中的最常见的2个词组合。\ndef get_top_n_bigram(corpus, n=None): vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] common_words = get_top_n_bigram(clean_news[\u0026#39;news\u0026#39;], 20) for word, freq in common_words: print(word, freq) df3 = pd.DataFrame(common_words, columns = [\u0026#39;news\u0026#39; , \u0026#39;count\u0026#39;]) df3.groupby(\u0026#39;news\u0026#39;).sum()[\u0026#39;count\u0026#39;].sort_values(ascending=False).iplot( kind=\u0026#39;bar\u0026#39;, yTitle=\u0026#39;Count\u0026#39;, linecolor=\u0026#39;black\u0026#39;, title=\u0026#39;Top 20 bigrams in news\u0026#39;) donald trump 25059 united states 18394 white house 15485 hillary clinton 9502 new york 8110 north korea 7053 president donald 6928 image via 6188 barack obama 5603 trump said 4816 prime minister 4753 president trump 4646 supreme court 4595 last year 4560 last week 4512 said statement 4425 fox news 4074 president obama 4065 islamic state 4014 national security 3858 发现：\n 如我们所担心的那样，考虑到王牌新闻的数量，我认为该模型的结果会有偏差 我们也可以看到朝鲜的新闻，我想这将是关于美国和NK之间的争端 fox 新闻也很少有新闻  def get_top_n_trigram(corpus, n=None): vec = CountVectorizer(ngram_range=(3, 3), stop_words=\u0026#39;english\u0026#39;).fit(corpus) bag_of_words = vec.transform(corpus) sum_words = bag_of_words.sum(axis=0) words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) return words_freq[:n] common_words = get_top_n_trigram(clean_news[\u0026#39;news\u0026#39;], 20) for word, freq in common_words: print(word, freq) df6 = pd.DataFrame(common_words, columns = [\u0026#39;news\u0026#39; , \u0026#39;count\u0026#39;]) df6.groupby(\u0026#39;news\u0026#39;).sum()[\u0026#39;count\u0026#39;].sort_values(ascending=False).iplot( kind=\u0026#39;bar\u0026#39;, yTitle=\u0026#39;Count\u0026#39;, linecolor=\u0026#39;black\u0026#39;, title=\u0026#39;Top 20 trigrams in news\u0026#39;) president donald trump 6808 president barack obama 3735 new york times 2034 donald trump realdonaldtrump 1790 reuters president donald 1476 black lives matter 1436 president united states 1096 white house said 1050 presidentelect donald trump 1043 new york city 1006 president vladimir putin 955 news century wire 951 national security adviser 898 affordable care act 868 director james comey 860 speaker paul ryan 851 fbi director james 778 state rex tillerson 775 secretary state rex 765 russian president vladimir 745 发现：\n 在弗洛伊德（Floyd）死后，有一个重要的新闻裁定美国媒体的“黑人生命问题”。 我们可以看到新闻已经覆盖了我们的数据。 关于死亡的假新闻很多。 其余新闻都与美国政治有关  虚假和真实新闻词云 虚假和真实新闻词云 让我们看看假新闻和真实新闻这两个词云.\ntext = fake_news[\u0026#34;news\u0026#34;] wordcloud = WordCloud( width = 3000, height = 2000, background_color = \u0026#39;black\u0026#39;, stopwords = STOPWORDS).generate(str(text)) fig = plt.figure( figsize = (40, 30), facecolor = \u0026#39;k\u0026#39;, edgecolor = \u0026#39;k\u0026#39;) plt.imshow(wordcloud, interpolation = \u0026#39;bilinear\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout(pad=0) plt.show() 发现\n 大多数虚假新闻都围绕唐纳德·特朗普和美国 还有关于隐私，互联网等的虚假新闻  text = true_news[\u0026#34;news\u0026#34;] wordcloud = WordCloud( width = 3000, height = 2000, background_color = \u0026#39;black\u0026#39;, stopwords = STOPWORDS).generate(str(text)) fig = plt.figure( figsize = (40, 30), facecolor = \u0026#39;k\u0026#39;, edgecolor = \u0026#39;k\u0026#39;) plt.imshow(wordcloud, interpolation = \u0026#39;bilinear\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout(pad=0) plt.show() 发现：\n 真正的新闻对共和党和俄罗斯并没有多大的帮助 有关于预算案的消息，军事消息也受到政府的消息报道  时间序列分析-虚假/真实新闻 让我们看一下在媒体上传播的真假新闻的时间表。\n# 创建时间的数量统计 fake=fake_news.groupby([\u0026#39;date\u0026#39;])[\u0026#39;output\u0026#39;].count() fake=pd.DataFrame(fake) true=true_news.groupby([\u0026#39;date\u0026#39;])[\u0026#39;output\u0026#39;].count() true=pd.DataFrame(true) #Plotting the time series graph fig = go.Figure() fig.add_trace(go.Scatter( x=true.index, y=true[\u0026#39;output\u0026#39;], name=\u0026#39;True\u0026#39;, line=dict(color=\u0026#39;blue\u0026#39;), opacity=0.8)) fig.add_trace(go.Scatter( x=fake.index, y=fake[\u0026#39;output\u0026#39;], name=\u0026#39;Fake\u0026#39;, line=dict(color=\u0026#39;red\u0026#39;), opacity=0.8)) fig.update_xaxes( rangeslider_visible=True, rangeselector=dict( buttons=list([ dict(count=1, label=\u0026#34;1m\u0026#34;, step=\u0026#34;month\u0026#34;, stepmode=\u0026#34;backward\u0026#34;), dict(count=6, label=\u0026#34;6m\u0026#34;, step=\u0026#34;month\u0026#34;, stepmode=\u0026#34;backward\u0026#34;), dict(count=1, label=\u0026#34;YTD\u0026#34;, step=\u0026#34;year\u0026#34;, stepmode=\u0026#34;todate\u0026#34;), dict(count=1, label=\u0026#34;1y\u0026#34;, step=\u0026#34;year\u0026#34;, stepmode=\u0026#34;backward\u0026#34;), dict(step=\u0026#34;all\u0026#34;) ]) ) ) fig.update_layout(title_text=\u0026#39;True and Fake News\u0026#39;,plot_bgcolor=\u0026#39;rgb(248, 248, 255)\u0026#39;,yaxis_title=\u0026#39;Value\u0026#39;) fig.show() 发现：\n 自2017年8月以来，真正的新闻就占据了主导地位。因为它们的出现率很高。这是一个好兆头 真实消息中的异常值高于虚假消息的异常值很少（2016年11月9日和2017年4月7日） 我们的数据集包含的假新闻比真实新闻多，因为我们可以看到我们没有整个2015年的真实新闻数据，因此，假新闻分类将比对真实新闻进行分类更为准确  Stemming \u0026amp; Vectorizing 词干化新闻\n词干提取是从变形词中得出词根的一种方法。 在这里，我们提取评论并将评论中的单词转换为其根词。 例如，\n Going-\u0026gt;go Finally-\u0026gt;fina 如果您注意到，则词根无需带有语义。 还有另一种称为“词法化”的技术，它可以将单词转换为具有语义含义的词根。  news_features=clean_news.copy() news_features=news_features[[\u0026#39;news\u0026#39;]].reset_index(drop=True) news_features.head() Out[31]: news 0\tdonald trump sends embarrassing new year’s eve... 1\tdrunk bragging trump staffer started russian c... 2\tsheriff david clarke becomes internet joke thr... 3\ttrump obsessed even obama’s name coded website... 4\tpope francis called donald trump christmas spe... In [32]: stop_words = set(stopwords.words(\u0026#34;english\u0026#34;)) # 词干化 ps = PorterStemmer() # 分词然后词干化 corpus = [] for i in range(0, len(news_features)): news = re.sub(\u0026#39;[^a-zA-Z]\u0026#39;, \u0026#39; \u0026#39;, news_features[\u0026#39;news\u0026#39;][i]) news= news.lower() news = news.split() news = [ps.stem(word) for word in news if not word in stop_words] news = \u0026#39; \u0026#39;.join(news) corpus.append(news) In [33]: corpus[1] Out[33]: \u0026#39;drunk brag trump staffer start russian collus investigationhous intellig committe chairman devin nune go bad day assumpt like mani us christoph steeledossi prompt russia investig lash depart justic fbi order protect trump happen dossier start investig accord document obtain new york timesform trump campaign advis georg papadopoulo drunk wine bar reveal knowledg russian opposit research hillari clintonon top papadopoulo covfef boy trump administr alleg much larger role none damn drunken fool wine bar coffe boy help arrang new york meet trump presid abdel fattah elsisi egypt two month elect known former aid set meet world leader trump team trump ran mere coffe boyin may papadopoulo reveal australian diplomat alexand downer russian offici shop around possibl dirt thendemocrat presidenti nomine hillari clinton exactli much mr papadopoulo said night kensington wine room australian alexand downer unclear report state two month later leak democrat email began appear onlin australian offici pass inform mr papadopoulo american counterpart accord four current former american foreign offici direct knowledg australian role papadopoulo plead guilti lie fbi cooper wit special counsel robert mueller teamthi presid badli script realiti tv showphoto win mcnameegetti imag\u0026#39; 这是现在的样子，因为计算机无法理解单词及其词义，我们需要将这些单词转换为1和0。为了对其进行编码，我们使用TFIDF。\nTFIDF(Term Frequency — Inverse Document Frequency) TF-IDF代表“词语频率-反向文档频率”。 这是一种量化文档中单词的技术，我们通常计算每个单词的权重，表示该单词在文档和语料库中的重要性。 此方法是信息检索和文本挖掘中广泛使用的技术。\n在这里，我们将bigram（两个单词）分开并考虑它们的总权重，而且我们只从新闻中提取前5000个单词。\ntfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(2,2)) # TFIDF矩阵 X= tfidf_vectorizer.fit_transform(news_features[\u0026#39;news\u0026#39;]) X.shape Out[34]: (44888, 5000) In [35]: #目标列 y=clean_news[\u0026#39;output\u0026#39;] In [36]: print(f\u0026#39;Original dataset shape : {Counter(y)}\u0026#39;) Original dataset shape : Counter({0: 23471, 1: 21417}) In [37]: ## 创建train或者test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state= 模型构建：Logistics Regression 假新闻分类器 因为我们已经成功处理了文本数据，所以这不仅仅是正常的机器学习问题。我们从稀疏矩阵中预测目标特征中的类。\ndef plot_confusion_matrix(cm, classes, normalize=False, title=\u0026#39;Confusion matrix\u0026#39;, cmap=plt.cm.Blues): \u0026#34;\u0026#34;\u0026#34; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \u0026#34;\u0026#34;\u0026#34; plt.imshow(cm, interpolation=\u0026#39;nearest\u0026#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype(\u0026#39;float\u0026#39;) / cm.sum(axis=1)[:, np.newaxis] print(\u0026#34;Normalized confusion matrix\u0026#34;) else: print(\u0026#39;Confusion matrix, without normalization\u0026#39;) thresh = cm.max() / 2. for i in range (cm.shape[0]): for j in range (cm.shape[1]): plt.text(j, i, cm[i, j], horizontalalignment=\u0026#34;center\u0026#34;, color=\u0026#34;white\u0026#34; if cm[i, j] \u0026gt; thresh else \u0026#34;black\u0026#34;) plt.tight_layout() plt.ylabel(\u0026#39;True label\u0026#39;) plt.xlabel(\u0026#39;Predicted label\u0026#39;) 模型选择 首先使用交叉验证选择最佳的成型模型。 让我们考虑所有分类算法并执行模型选择过程 注意：我没有在此算法中包含SVM，因为在设备中进行培训需要花费很多时间\n# 创建分类器 logreg_cv = LogisticRegression(random_state=0) dt_cv=DecisionTreeClassifier() knn_cv=KNeighborsClassifier() nb_cv=MultinomialNB(alpha=0.1) cv_dict = {0: \u0026#39;Logistic Regression\u0026#39;, 1: \u0026#39;Decision Tree\u0026#39;,2:\u0026#39;KNN\u0026#39;,3:\u0026#39;Naive Bayes\u0026#39;} cv_models=[logreg_cv,dt_cv,knn_cv,nb_cv] # 准确率 for i,model in enumerate(cv_models): print(\u0026#34;{} Test Accuracy: {}\u0026#34;.format(cv_dict[i],cross_val_score(model, X, y, cv=10, scoring =\u0026#39;accuracy\u0026#39;).mean())) Logistic Regression Test Accuracy: 0.9660040199274997 Decision Tree Test Accuracy: 0.9347925003047657 KNN Test Accuracy: 0.613172841991654 Naive Bayes Test Accuracy: 0.9373328405462511 参数调整 param_grid = {\u0026#39;C\u0026#39;: np.logspace(-4, 4, 50), \u0026#39;penalty\u0026#39;:[\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;]} clf = GridSearchCV(LogisticRegression(random_state=0), param_grid,cv=5, verbose=0,n_jobs=-1) best_model = clf.fit(X_train,y_train) print(best_model.best_estimator_) print(\u0026#34;The mean accuracy of the model is:\u0026#34;,best_model.score(X_test,y_test)) LogisticRegression(C=24.420530945486497, random_state=0) The mean accuracy of the model is: 0.9803065407235787 In [42]: logreg = LogisticRegression(C=24.420530945486497, random_state=0) logreg.fit(X_train, y_train) y_pred = logreg.predict(X_test) print(\u0026#39;Accuracy of logistic regression classifier on test set: {:.2f}\u0026#39;.format(logreg.score(X_test, y_test))) Accuracy of logistic regression classifier on test set: 0.98 混淆矩阵 cm = metrics.confusion_matrix(y_test, y_pred) plot_confusion_matrix(cm, classes=[\u0026#39;Fake\u0026#39;,\u0026#39;True\u0026#39;]) Confusion matrix, without normalization 分类报告 print(\u0026#34;Classification Report:\\n\u0026#34;,classification_report(y_test, y_pred)) Classification Report: precision recall f1-score support 0 0.98 0.98 0.98 5892 1 0.98 0.98 0.98 5330 accuracy 0.98 11222 macro avg 0.98 0.98 0.98 11222 weighted avg 0.98 0.98 0.98 11222 ROC-AUC Curve 这是一条非常重要的曲线，我们可以根据客观标准来决定要设置的阈值。\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test)) fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1]) plt.figure() plt.plot(fpr, tpr, label=\u0026#39;Logistic Regression (area = %0.2f)\u0026#39; % logit_roc_auc) plt.plot([0, 1], [0, 1],\u0026#39;r--\u0026#39;) plt.xlim([-0.01, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver operating characteristic\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.show() 深度学习模型-LSTM 在本部分中，我们使用神经网络来预测给定新闻是否为假新闻。\n我们不会使用像ANN这样的普通神经网络来分类，而是使用LSTM（长期短期记忆）来帮助包含序列信息。长期短期记忆（LSTM）网络是一种能够学习序列依赖的递归神经网络。 序列预测问题。 这是机器翻译，语音识别等复杂问题域中所需的行为。\n嵌入层-One Hot编码 # 词汇表大小 voc_size=10000 #One hot 编码 onehot_repr=[one_hot(words,voc_size)for words in corpus] 对齐序列 clean_news[\u0026#39;word_count\u0026#39;].describe() Out[49]: count 44888.000000 mean 237.051907 std 198.796232 min 3.000000 25% 124.000000 50% 210.000000 75% 294.000000 max 4831.000000 Name: word_count, dtype: float64 In [50]: # 设置序列长度 sent_length=5000 # 补齐 embedded_docs=pad_sequences(onehot_repr,padding=\u0026#39;pre\u0026#39;,maxlen=sent_length) print(embedded_docs) [[ 0 0 0 ... 3061 6048 7061] [ 0 0 0 ... 5721 4842 7061] [ 0 0 0 ... 5552 4139 7061] ... [ 0 0 0 ... 7960 9583 8051] [ 0 0 0 ... 5323 5417 3542] [ 0 0 0 ... 8286 7502 4272]] LSTM模型 #构建lstm模型 embedding_vector_features=40 model=Sequential() model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length)) model.add(Dropout(0.3)) model.add(LSTM(100)) # 100 model.add(Dropout(0.3)) model.add(Dense(1,activation=\u0026#39;sigmoid\u0026#39;)) # 编译模型 model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;,optimizer=\u0026#39;adam\u0026#39;,metrics=[\u0026#39;accuracy\u0026#39;]) print(model.summary()) Model: \u0026#34;sequential\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= embedding (Embedding) (None, 5000, 40) 400000 _________________________________________________________________ dropout (Dropout) (None, 5000, 40) 0 _________________________________________________________________ lstm (LSTM) (None, 100) 56400 _________________________________________________________________ dropout_1 (Dropout) (None, 100) 0 _________________________________________________________________ dense (Dense) (None, 1) 101 ================================================================= Total params: 456,501 Trainable params: 456,501 Non-trainable params: 0 _________________________________________________________________ None In [52]: len(embedded_docs),y.shape Out[52]: (44888, (44888,)) 模型训练 # 转换X和y X_final=np.array(embedded_docs) y_final=np.array(y) # X和y大小 X_final.shape,y_final.shape Out[53]: ((44888, 5000), (44888,)) In [*]: # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42) # epochs设置为10 batch size大小为64 model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64) Epoch 1/10 470/470 [==============================] - 125s 267ms/step - loss: 0.2214 - accuracy: 0.9109 - val_loss: 0.0735 - val_accuracy: 0.9768 Epoch 2/10 470/470 [==============================] - 126s 269ms/step - loss: 0.0750 - accuracy: 0.9764 - val_loss: 0.0686 - val_accuracy: 0.9820 Epoch 3/10 470/470 [==============================] - 126s 268ms/step - loss: 0.0538 - accuracy: 0.9837 - val_loss: 0.0550 - val_accuracy: 0.9849 Epoch 4/10 303/470 [==================\u0026gt;...........] - ETA: 38s - loss: 0.0404 - accuracy: 0.9881 模型评估 # 测试集数据预测 y_pred=model.predict_classes(X_test) # 混淆矩阵 # confusion_matrix(y_test,y_pred) cm = metrics.confusion_matrix(y_test, y_pred) plot_confusion_matrix(cm,classes=[\u0026#39;Fake\u0026#39;,\u0026#39;True\u0026#39;]) accuracy_score(y_test,y_pred) Out[56]: 0.9800864047522614 In [57]: print(classification_report(y_test,y_pred)) precision recall f1-score support 0 0.98 0.98 0.98 7777 1 0.98 0.98 0.98 7037 accuracy 0.98 14814 macro avg 0.98 0.98 0.98 14814 weighted avg 0.98 0.98 0.98 14814 Bidirectional LSTM Bi-LSTM是标准LSTM的扩展，具有两个独立的RNN。 普通的LSTM是单向的，它无法知道将来的单词，而在Bi-LSTM中，我们可以预测单词的未来使用，因为存在从另一RNN层反向传递的反向信息。\n与LSTM相比，代码只做了一个更改，这里我们使用Bidirectional（）函数并在内部调用LSTM。\nembedding_vector_features=40 model1=Sequential() model1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length)) model1.add(Bidirectional(LSTM(100))) # Bidirectional LSTM layer model1.add(Dropout(0.3)) model1.add(Dense(1,activation=\u0026#39;sigmoid\u0026#39;)) model1.compile(loss=\u0026#39;binary_crossentropy\u0026#39;,optimizer=\u0026#39;adam\u0026#39;,metrics=[\u0026#39;accuracy\u0026#39;]) print(model1.summary()) Model: \u0026#34;sequential_1\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= embedding_1 (Embedding) (None, 5000, 40) 400000 _________________________________________________________________ bidirectional (Bidirectional (None, 200) 112800 _________________________________________________________________ dropout_2 (Dropout) (None, 200) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 201 ================================================================= Total params: 513,001 Trainable params: 513,001 Non-trainable params: 0 _________________________________________________________________ None 双向LSTM模型的拟合与评估 现在让我们将双向LSTM模型拟合到我们拥有的数据之前具有相同参数的数据上。\n# 测试集 y_pred1=model1.predict_classes(X_test) # 混淆矩阵 cm = metrics.confusion_matrix(y_test, y_pred1) plot_confusion_matrix(cm,classes=[\u0026#39;Fake\u0026#39;,\u0026#39;True\u0026#39;]) In [62]: # 准确率 accuracy_score(y_test,y_pred1) Out[62]: 0.615161333873363 In [61]: # 分类报告 print(classification_report(y_test,y_pred1)) precision recall f1-score support 0 0.61 0.74 0.67 7777 1 0.62 0.48 0.54 7037 accuracy 0.62 14814 macro avg 0.62 0.61 0.61 14814 weighted avg 0.62 0.62 0.61 14814 结论 我们已经在处理数据和构建模型方面进行了主流工作。 我们本可以在向量化文本数据的同时沉迷于更改ngram。 我们拿了2个字并将其向量化。 您可以通过同时考虑1和2个单词来检查Shreta在同一数据集上的工作，她在LSTM和Bi-LSTM网络的帮助下获得了更好的结果。让我们讨论数据集的一般见解。\n大多数虚假新闻都被选举新闻和特朗普所包围。 考虑到2020年美国大选。有传播假新闻的机会，将非常需要这些技术的应用。 假新闻目前在这种大流行情况中根深蒂固，以打政治，恐吓人们并强迫他们购买商品 大多数新闻来自路透社。 我们不知道这种新闻媒体是否受到政治影响。 因此，我们应始终考虑新闻来源，以查找新闻是虚假还是真实。\n最后欢迎大家关注我们的公众号：ChallengeHub，加入ChallengeHub粉丝群，共同探讨，共同学习，共同进步！！！ ","permalink":"/posts/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%9F%BA%E4%BA%8Elstm%E7%9A%84%E7%BE%8E%E5%9B%BD%E5%A4%A7%E9%80%89%E7%9A%84%E6%96%B0%E9%97%BB%E7%9C%9F%E5%81%87%E5%88%86%E7%B1%BB/","series":["深度学习课程"],"tags":["深度学习课程"],"title":"基于LSTM的美国大选的新闻真假分类【NLP 新年开胃菜】"},{"categories":["技术博客"],"content":"#我们回归下，如下图所示Graphs用来做传统机器学习任务的流程为：给定输入Graph,然后用来提取节点，边以及图级别的特征，之后学习模型（比如SVM,NN等等），最后将特征映射为标签。 1 引言 图表示学习可以减轻特征工程的需求，表示学习可以自动学习Graph的特征，用于下游任务，熟悉NLP的同学比较清楚，传统文本特征构建依赖于统计手段来实现，冗余且效果有限，在词向量Word2Vec和预训练模型Bert等出现之后，文本表示变得更为便利且效果强大，自此万物皆可Embedding。\n图的表示学习的目的就是获得独立于不同任务的高效特征，通俗点讲就是能够针对不同任务学习得到适合任务的嵌入表示。 Node Embedding的目的就是能够将节点映射到不同的embedding空间：\n 节点间的embedding的相似性可以表示了节点间在网络的相似性：如果两个节点之间存在边，那么两个节点越相似 节点的Embedding能够编码网络信息 可以用于下游任务，比如节点分类，边的预测，图的分类等 下面是一个Zachary\u0026rsquo;s Karate Club Network的2维节点Embedding展示：   2 节点嵌入：编码和解码 这一节我们主要掌握下如何学习节点的嵌入向量\n2.1 构建输入-图 首先，假设我们有一个图$G$：\n $V$代表节点的集合 $A$ 代表链接矩阵 为了方便起见，我们默认认为节点没有其他的额外特征，如下图所示：   2.2 学习目标-Node Embedding 我们的目标是能够学习到节点的嵌入表示，这种节点嵌入的相似性能够近似节点在图中的相似性。 图中$ENC(u)$代表对节点$u$进行编码表示得到$z_{u}$,同样$ENC(v)$代表对节点$u$进行编码表示得到$z_{v}$\n2.3 学习方法-Encoder和Decoder  **Encoder**能够将节点映射成向量 定义一个节点相似性函数(比如节点在原始图中的相似性评估方法) Decoder DEC能够将节点向量映射成相似性分数 优化编码器的参数： 原始网络的相似性：$similarity(u,v)$ $\\approx$ 节点嵌入的相似性 ： $z_{v}^Tz_{u}$  其中有两个非常关键因素：编码和相似性函数。编码器能够学习到节点的向量，相似性函数能够提供我们优化的目标，如果两个节点在图中存在边或者距离越近，那么它们对应的节点向量在向量空间中相似性也会越大。 如何选择定义节点相似性的方法是关键的地方，我们考虑下什么情况下，两个节点的相似性比较高？\n 是否存在边或者连接 是否共享邻居节点 是否包括相似的属性特征 接下来我们通过随机游走（Random Walks）来学习节点相似性，以及优化节点的嵌入向量。  3 Random Walk Node Embeddings的计算方法之一就是Random Walk，为了方面起见我们定义和引出下面的符号，以便统一解释。\n3.1 符号 为了方面接下来的公式推导，我们统一下符号标记：\n 向量 $z_{u}$:  u节点的嵌入向量\n  概率$P(v|z_{u})$：  从节点$u$出发通过随机游走访问节点$v$的概率\n  Softmax函数：通过softmax函数一作用，一个K维向量就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。 $\\sigma(z){i}=\\frac{e^{Z{i}}}{\\sum_{j=1}^Ke^{Z_{j}}}$ Sigmoid函数： $S(x)=\\frac{1}{1+e^{-x}}$将一个数映射到（0,1）的区间  3.2 Random Walk 定义 给定一个Graph和一个起始点，我们选择随机选择该点的邻居节点，并且移动到该邻居节点；然后我们根据当前节点随机选择一个邻居节点，然后移动到该邻居节点，重复上述步骤\u0026hellip;通过这种随机游走访问Graph节点的方式可以产生随机序列。下图描述了一个随机游走的实例，第一步从起始节点出发移动到节点5，第二步从节点5移动节点8，第三步从节点8移动节点9，第四步又从节点9移动到节点8，之后第5步移动到节点11，这样访问路径构成一个序列：\nstart_node-\u0026gt;5-\u0026gt;8-\u0026gt;9-\u0026gt;8-\u0026gt;11 3.3 Random Walk如何得到Node Embeddings 相信大家对下面的公式不会感到陌生，一般我们衡量两个向量的相似性时可以通过两个向量点积计算得到，相似地两个节点同时出现在同一随机游走序列的概率可以用如下表示： 所以我们通过Rankdom Walk得到Node Embedding可以通过两个步骤： (1) 通过某种随机游走策略$R$（下文将会介绍）得到从节点$u$出发访问到节点$v$的路径，然后可以估计出节点$u$和$v$同时出现的概率 (2) 优化节点的embeddings表示，使得节点的嵌入表示能够表达或者近似我们上述随机游走得到的统计 回想一下我们机器学习或者深度学习的各种任务，无非就是在优化一个目标，一个近似目标，我们可以笼统地将我们输入表示成X，然后通过方法f来近似y。说到这里上面两个步骤特别像NLP中Glove向量的优化步骤，首先我们统计两个词在语料中共现概率，然后通过词向量来近似共现概率。\n","permalink":"/posts/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C03-node-embeddings/","series":["图神经网络"],"tags":["图神经网络"],"title":"图神经网络（03）-Node Embeddings"},{"categories":["ChallengeHub"],"content":" ChallengeHub:A community dedicated to AI knowledge sharing\n 成员信息  致Great，毕业于中国人民大学，就职于某研究院，算法工程师； lrhao，就职于某互联网大厂数据分析； 不是吧阿sir，华东师范大学研二学生，目前某互联网大厂算法实习； WintoMT，中南大学研二学生； starry，毕业于同济大学，就职于某互联网大厂，算法工程师  成员荣誉  2019年厦门国际银行数创金融杯 冠军 2021年水利知识图谱构建挑战赛 亚军 2020年CCF华为serverless负载预测 亚军 2020年新网银行用户行为体识别 亚军 2020年众安科技用户骗保行为识别 亚军 2020年中国电信翼支付风险用户识别 季军 2020年人工智能图像分类应用挑战赛 季军 2020年长三角创同体数据挖掘竞赛 季军 2020年CCF企业风险非法集资预测 季军 2019年中国银联用户行为购买预测 亚军  联系邮箱 challengehub@126.com\n","permalink":"/about/challengehub/","series":null,"tags":["ChallengeHub"],"title":"ChallengeHub"},{"categories":["技术博客"],"content":"本文展示了如果使用 scikit-learn 使用。\nsklearn (scikit-learn) 是基于 Python 语言的机器学习工具\n 官方链接：https://scikit-learn.org/stable/\n 安装  通过pip安装  $ pip install -U scikit-learn  通过conda安装  $ conda install -c intel scikit-learn 实例 from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier(random_state=0) X = [[ 1, 2, 3], # 2 samples, 3 features [11, 12, 13]] y = [0, 1] # classes of each sample clf.fit(X, y) RandomForestClassifier(random_state=0) ","permalink":"/posts/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/scikit-learn/","series":["Scikit-Learn系列教程"],"tags":["scikit-learn"],"title":"Scikit-Learn"},{"categories":["比赛推送"],"content":"KDD 2021 赛题 赛题1 基于多数据集的时间序列异常检测  Multi-dataset Time Series Anomaly Detection https://compete.hexagon-ml.com/practice/competition/39/\n 背景描述-时间序列异常检测 近年来，SIGKDD以及其他数据挖掘，机器学习和数据库会议上出现了有关时间序列异常检测的论文。这些论文中的大多数都在一个或多个基准数据集中进行测试，包括由NASA，Yahoo，Numenta和清华-OMNI 等创建的数据集。\n尽管社区应该非常赞赏这些团队共享数据的努力，但最近的几篇论文[a]认为这些数据集不适用于衡量异常检测的进展。\n简而言之，反对使用这些数据集的两个最引人注目的论据是：\n冗余性：几乎所有上述基准数据集都可以完美解决，而无需查看任何训练数据，并且使用已有十年历史的算法。 标签错误：永远不能完全消除错误检测基准错误贴标签的可能性。但是，上面提到的某些数据集在基本事实中似乎有大量假阳性和假阴性。已经发表了一些论文，认为方法A比方法B更好，因为它在基准X上的准确性要高5％。但是，仔细检查基准X可以发现，有25％以上的标签是错误的，这个数字使标签A的准确性相形见war。声称所比较算法之间的差异。\n除了上面列出的问题以及文件重叠的可能性外，我们认为社区还存在一系列不合适的基准。考虑到这一点，作为比赛的一部分，我们为时间序列异常检测创建了新的基准。\n为此竞赛创建的基准数据集旨在缓解此问题。重要的是要注意我们的主张是“缓解”，而不是“解决”。我们认为，非常有很多研究者本着CASP [d]的精神来解决这个问题将是很棒的。\n同时，作为这一挑战的一部分的200个数据集反映了20多年研究时间序列异常检测文献并收集数据集的工作。除了这场竞赛的本身，我们希望它们可以在未来几年中为社区提供资源，并激发对异常检测评估的更深刻的思考。\n赛题奖励  一等奖：$ 2000 USD 二等奖：$ 1000 USD 三等奖：$ 500美元 对于排名前15位的参与者，我们将提供具有等级的证书。 对于所有其他参与者，我们将提供参赛证书（阳光普照奖）  赛题时间轴  阶段1:2021年3月15日-2021年4月7日 阶段2:2021年4月7日-20210年6月1日  赛题任务与数据 赛题任务：预测时间序列中异常发生的位置\n数据    姓名 下载     提交样例 [下载](https://compete.hexagon-ml.com/media/data/multi-dataset-time-series-anomaly-detection-39/submissionsample.csv)    数据 [下载](https://compete.hexagon-ml.com/media/data/multi-dataset-time-series-anomaly-detection-39/data.zip)    文件格式\n这些文件使用命名约定，该约定在测试和训练之间提供了分隔。\n_ \u0026lt;名称\u0026gt; _ \u0026lt;拆分号\u0026gt; .txt\n例如004_UCR_Anomaly_2500.txt。 此处split-number = 2500表示从2500开始存在异常。\n样本提交文件\n提示：\n提交文件的column标头应“完全”匹配预期的格式。例如，编号，位置\n行数应与总计数完全匹配（第一阶段：25行，第二阶段：200行）\nlocation的值是一个整数。\n第一阶段 数据部分将提供25个时间序列文件以及示例提交文件。 这将是一个培训阶段，在进入第二阶段时将清除排行榜。\n第二阶段\n比赛第二阶段将提供200个时间序列文件，包括第一阶段的前25个文件。\n提交评估 我们在异常范围的两侧添加了+/- 100个位置，以奖励正确的答案。\n例子\n有200个文件，对于每个正确的答案，您将获得1分，对于错误的答案将获得0分。您可以获得的最高分数是100％（只要您使用算法在代码中执行此操作，就无需人工标记）。如果我们怀疑有任何参与者\n赛题可以提交了，别等待了~ 赛题2 城市大脑挑战-交通网络调度  City Brain Challenge http://www.yunqiacademy.org/\n 赛题背景 没有人喜欢被卡在城市交通中。尽管我们在城市中观察到许多车辆，但交通拥堵的原因仍不清楚。是因为车辆数量超出了城市的承载能力，还是因为我们未能以最大承载能力利用道路网络？\n以世界上最大的两个城市为例。东京和纽约市的交通拥堵指数排名相似。但是，值得注意的是，东京的注册车辆比纽约市多50％，而东京的信号交叉口仅比纽约市多15％，道路长度比纽约多30％。（东京：注册车辆313万， 交通信号15,000  ，道路24,650公里。 纽约市注册车辆219万， 交通信号13,000  ，道路18,684 km。 ）      为什么东京可以提供比纽约更多的车辆服务？纽约市是否以最大载客量运营交通？作为数据科学家，我们邀请您协调交通，并根据城市规模的路网及其交通需求找到最大交通容量。\n比赛描述 在这一挑战中，我们将为您提供一个城市规模的道路网络，其交通需求来自真实的交通数据。您将负责协调信号交叉口的交通，同时将延迟指数保持在预定义的阈值以下。我们将增加流量需求，并查看您的协调模型是否仍然可以凑效。\n为了促进您的方法开发，我们将首次发布City Brain Open Research Platform。该平台包含一个城市规模的交通模拟环境和一个具有多核计算机的云计算集群。 时间线  报名 4/1/2021  参赛选手熟悉区域交通的数据以及熟悉模拟环境。\n 参赛 5/1/2021  参与者将进行城市规模的交通协调。可以处理更大流量需求的团队将进入最后一轮。\n 最终提交 6/1/2021  提供大规模的云计算平台。团队将开发方法来处理城市范围内各种未知的交通流量。\n 比赛结束 7/1/2021  比赛奖励 奖金：\n 第一名：$ 3000 第二名：$ 2000 第三名：$ 1000 第四至第十名：$ 500  研究支持：\n 大规模计算资源 潜在的现实世界级研究实验 纸质出版物的其他支持  赛题3 大型图机器学习比赛： OGB-LSC  OGB Large-Scale Challenge https://ogb.stanford.edu/kddcup2021/\n 为什么要举行大型图ML比赛？ 由于在实际应用中普遍使用图结构化数据，因此图上的机器学习（ML）近年来引起了极大的关注。现代应用领域包括网络规模的社交网络，推荐系统，超链接的网络文档，知识图谱（KGs），以及通过不断增长的科学计算生成的分子模拟数据。这些域涉及具有数十亿个边的大规模图形或具有数百万个图形的数据集。大规模部署准确的图ML将产生巨大的实际影响，从而实现更好的推荐结果，改进的Web文档搜索，更全面的KG以及基于ML的准确药物和材料发现。\n然而，社区在大规模图形ML中发展最新技术的努力非常有限。实际上，处理大规模图具有挑战性，特别是对于最先进的表达性图神经网络（GNN），因为它们会根据来自许多其他节点的信息对每个节点进行预测。要有效地大规模训练这些模型，就需要复杂的算法，而这些算法远远超出了基于iid数据的标准SGD。最近，研究人员通过显着简化GNN来提高模型可伸缩性，这不可避免地限制了它们的表达能力。\n但是，在深度学习中，一遍又一遍地表明，人们需要大型的表达模型并在大数据上对其进行训练，以实现最佳性能。在图ML中，趋势是相反的-模型变得简化且表达能力较弱，因此无法缩放到大图。因此，存在着巨大的机会来移动社区以使用现实的和大规模的图形数据集，并将领域的状态向前移动到需要的地方。\nOGB-LSC概述 在这里，我们提出了一个大型图ML竞赛，即OGB大型挑战赛（OGB-LSC），以鼓励开发适用于海量现代数据集的最新图ML模型。具体来说，我们提供了三个数据集：MAG240M-LSC ，WikiKG90M-LSC 和PCQM4M-LSC ，它们的规模空前大，并且分别覆盖了节点，链接和图形级别的预测。 每个数据集提供一个独立的任务，优胜者将分别为每个数据集选择。 我们将宣布每个数据集的前3名获胜团队（总共9个获胜团队），他们将有机会在KDD Cup研讨会上展示他们的解决方案。 下面提供了三个OGB-LSC数据集的说明性概述：  **MAG240M-LSC **是一个异构的学术图，其任务是预测位于异构图中的论文的主题区域（节点分类）。 **WikiKG90M-LSC **是一个知识图，其任务是估算缺少的三元组（链接预测）。 **PCQM4M-LSC **是量子化学数据集，其任务是预测给定分子的重要分子特性，即HOMO-LUMO间隙（图形回归）。  对于每个数据集，我们都会仔细设计其预测任务和数据拆分，以便在任务上实现较高的预测性能将直接影响相应的应用程序。每个数据集页面中都提供了更多详细信息。\n数据集统计信息和基本信息总结如下，表明我们的数据集非常大。 所有这些数据集都可以使用我们的ogbPython包 下载并准备。 模型评估和测试提交文件的准备工作也由我们的软件包处理。 用法在每个数据集页面中都有描述。请通过以下方式安装/更新：\n在我们的**论文中 **，我们进一步对每个数据集进行了广泛的基线分析，大规模实现了简单的基线模型以及高级的表达模型。我们发现，尽管需要更多的努力来进行扩展，但先进的表达模型确实会从大数据中受益，并且明显优于易于扩展的简单基线模型。我们所有的基准代码均已**公开提供， **以方便公众研究。\npip install -U ogb # Make sure below prints the required package version for the dataset you are working on. python -c \u0026quot;import ogb; print(ogb.__version__)\u0026quot; 总体而言，我们的KDD杯将鼓励社区开发和扩展表达性图ML模型，这可以在各个领域取得重大突破。我们希望在2021年KDD杯上的OGB-LSC能够成为图ML领域的“ ImageNet大规模视觉识别挑战”，鼓励社区致力于现实和大规模的图数据集，并显着提高现状-艺术。 OGB-LSC数据集论文 \nKaggle新赛题 赛题1 Shopee - Price Match Guarantee 通过两个图像确定两个产品是否相同\n https://www.kaggle.com/c/shopee-product-matching/overview/description\n 评估指标 采用F1-Score，参赛作品将根据其平均F1分数 进行评估。均值以样本方式计算，这意味着将为每个预测行计算F1分数，然后取平均值。\n选手必须创建一个以空格分隔的列表，posting_id该列表与该posting_id列中的发布匹配的所有。帖子总是自我匹配的。匹配个数上限为50，因此预测50场以上的对比赛没有好处。\n该文件应有一个标题，名为submission.csv，如下所示：\nposting_id,matches test_123,test_123 test_456,test_456 test_789 您应该预测每个比赛posting_id。例如，如果您认为A与B和C相匹配，则A,A B C还应包含B,B A C和C,C A B。\n时间线  2021 年3月8日-比赛开始日期 2021 年5月3日-报名截止日期。您必须在此日期之前接受比赛规则才能参加比赛。 2021 年5月3日-合并团队截止日期。这是参与者可以加入或合并团队的最后一天。 2021 年5月10日-最终提交截止日期。  奖金  第一名-$ 15,000 第二名-$ 10,000 第三名-$ 5,000  数据集 在大型数据集中查找近重复项是许多在线业务的重要问题。对于Shopee，日常用户可以上传自己的图像并撰写自己的产品说明，从而增加了挑战。您的任务是确定哪些产品已重复发布。相关产品之间的差异可能微妙，而相同产品的照片则可能千差万别！\n由于这是一场代码竞赛，因此仅发布测试集的前几行/图像；其余的仅在提交笔记本后才对您的笔记本可用。预计将在隐藏的测试集中找到大约70,000张图像。提供的一些测试行和图像旨在说明隐藏的测试集格式和文件夹结构。\n **[train / test] .csv-**训练集数据。每行包含一次过帐的数据。多个订单可能具有完全相同的图像ID，但标题不同，反之亦然。    posting_id -发布的ID码。\n  image -图片ID / md5sum。\n  image_phash-图像的感知哈希 。\n  title -发布的产品说明。\n  label_group-映射到同一产品的所有发布的ID码。未提供测试集。\n  [train / test] images-与发布相关的图像。\n**sample_submission.csv-**格式正确的示例提交文件。\n  posting_id -发布的ID码。\n  matches-以空格分隔的与此发布匹配的所有发布ID的列表。posting是自我匹配，匹配个数上限为50，因此无需预测超过50个数。\n  赛题2 Bristol-Myers Squibb – Molecular Translation 化学分子图像转换为化学结构序列 https://www.kaggle.com/c/bms-molecular-translation\n背景描述 在技术进步的世界中，有时最好，最简单的工具仍然是纸和笔。有机化学家经常利用骨架式（Skeletal formula）来绘制分子工作，骨架式是一个世纪以来使用的结构符号。最近的出版物还用机器可读的化学描述（InChI）进行了注释，但是数十年的扫描文档无法自动搜索特定的化学描述。在机器学习的帮助下，光学化学结构的自动识别可以加快研发的速度。\n不幸的是，大多数公共数据集太小，无法支持现代机器学习模型。现有工具只能在最佳条件下产生90％的精度。历史来源通常会在某种程度上破坏图像，从而将性能降低到接近零。在这些情况下，需要耗时的手动操作才能将扫描的化学结构图像可靠地转换为机器可读格式。\n百时美施贵宝公司是一家全球性生物制药公司，致力于通过科学改变患者的生活。他们的任务是发现，开发和提供可帮助患者战胜严重疾病的创新药物。\n在这场比赛中，您将解释旧的化学图像。通过访问由Bristol-Myers Squibb生成的大量合成图像数据，您可以将图像转换回标注为InChI文本的基础化学结构。\n整理化学文献的工具将对研究人员带来重大好处。如果成功，您将帮助化学家扩大进行集体化学研究的机会。反过来，通过避免重复先前发表的化学方法，并通过挖掘大数据集来识别新趋势，这将加快许多关键领域的研发工作。\n评估指标 根据您提交的InChi字符串与地面真实InChi值之间的平均Levenshtein距离 评估提交的内容。\n提交文件 对于image_id测试集中的每一个，您必须在相应的图像中预测分子的InChi字符串。该文件应包含标头，并具有以下格式：\nimage_id,InChI 00000d2a601c,InChI=1S/H2O/h1H2 00001f7fc849,InChI=1S/H2O/h1H2 000037687605,InChI=1S/H2O/h1H2 etc. 时间轴  2021 年3月2日-比赛开始日期 2021 年5月26日-报名截止日期。您必须在此日期之前接受比赛规则才能参加比赛。 2021 年5月26日-合并团队截止日期。这是参与者可以加入或合并团队的最后一天。 2021 年6月2日-最终提交截止日期。  奖金  第一名-$ 25,000 第二名-$ 15,000 第三名-$ 10,000  数据集 在本次比赛中，您将获得化学品的图像，目的是预测图像中相应的国际化学品识别码 （InChI）文本字符串。所提供的图像（训练数据和测试数据中的图像）都可以旋转到不同的角度，具有不同的分辨率，并且具有不同的噪声水平。\n**注意：**此数据集中总共约有4m张图像。解压缩下载的数据将花费时间。\n train / -训练图像，由3级文件夹结构排列image_id  test / -测试图像，与文件夹位于相同的文件夹结构中train/ train_labels.csv-训练图像的地面真相InChi标签 sample_submission.csv-格式正确的样本提交文件  ","permalink":"/posts/%E6%AF%94%E8%B5%9B%E6%8E%A8%E9%80%81/kaggle%E9%A1%B6%E7%BA%A7%E8%B5%9B%E4%BA%8B%E7%AD%89%E4%BD%A0%E6%9D%A5%E6%8E%92%E4%BD%8D/","series":["比赛推送"],"tags":["比赛推送"],"title":"【比赛推送】KDD 2021/ Kaggle 顶级赛事等你来排位"},{"categories":["招聘信息"],"content":"1 京东探索研究院（社招）  京东成立探索研究院，将深耕“人工智能”、“量子计算”、“数据科学、工程与管理”、“去中心化计算”、“技术伦理道德”、“科学与艺术”六大技术领域，从基础理论层面实现颠覆式创新，聚焦于打造产业数智化首个源头性科技高地，成为比肩国际领先科研机构的前沿技术基地，通过不断努力，在世界舞台上展现一家中国科技公司的担当、风范。\n  现在，京东探索研究院将在量子计算、机器感知、理论算法方向招募世界顶级科学家，组建首批科研团队，同时励众更多“青年科学家”加入。改变世界，从此刻开始。\n 2 蚂蚁集团（春招实习） 内推方式：\n大佬朋友的组，同学们抓紧上车 2022校招实习开始了，扫码内推，或者直接发简历到yongliang.wyl@antgroup.com。 阿里云 (暑期实习) 【阿里云-分布式块存储研发-暑期实习直推】（实习面向2022年毕业的学生） 一、团队介绍： 阿里云块存储团队，目前提供了中国最大的线上云存储服务。团队管理高效，技术积累丰富，培养体系成熟，在杭州、北京均设有团队。\n二、岗位职责： 1.负责阿里云分布式块存储万亿级别业务的架构设计、研发，提供高稳定性、高可靠性的分布式存储服务； 2.在现有系统设计基础上，完善优化现有系统架构设计，提升系统稳定性及可靠性； 3.为现有系统的稳定可靠运行，提供技术支持；\n三、岗位要求：\n 本科及以上学历在读学生，计算机、软件工程或相关专业或多年分布式开发相关经验者； 熟悉Linux系统开发环境，具有良好的代码风格和质量意识，熟悉C/C++等编程语言（其他语言也可以考虑，但需对C/C++有一定了解）; 有优秀的发现及解决问题的能力，良好的沟通能力及团队合作意识; 对分布式系统的架构和原理有深入了解者优先考虑； 有编程竞赛经验者或分布式系统开发经验者优先考虑。  四、简历投递 邮箱：liangliang.sll@alibaba-inc.com qq：495096408 直接发送简历到邮箱或者qq联系本人\n3 Moka(社招) 4 字节跳动2021春季校园招聘全面启动！（校招）  ps：信息来自友圈，有疑问或者问题可以联系后台进行处理\n ","permalink":"/posts/%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/kaggle%E9%A1%B6%E7%BA%A7%E8%B5%9B%E4%BA%8B%E7%AD%89%E4%BD%A0%E6%9D%A5%E6%8E%92%E4%BD%8D/","series":["招聘信息"],"tags":["招聘信息"],"title":"【阿里，字节，Moka】招聘信息汇总（ML、CV、NLP）"}]